{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN_MLC(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels=16):\n",
    "        super(GCN_MLC, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)  # Additional layer for deeper learning\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)  # Dropout for regularization\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class SAGE_MLC(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels=16):\n",
    "        super(SAGE_MLC, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data(device):\n",
    "    # Load data, assuming the paths are correct\n",
    "    X = torch.load('results/X_32.pt')\n",
    "    Y = torch.load('results/Y.pt')\n",
    "    A = scipy.sparse.load_npz('results/A/A_final.npz')\n",
    "\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(A)\n",
    "    edge_index = edge_index.to(device)\n",
    "    edge_weight = edge_weight.to(device)\n",
    "\n",
    "    # Convert X and Y to torch tensors if they are numpy arrays\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.tensor(X, dtype=torch.float).to(device)\n",
    "    else:\n",
    "        X = X.to(device)\n",
    "\n",
    "    if isinstance(Y, np.ndarray):\n",
    "        Y = torch.tensor(Y, dtype=torch.float).to(device)  # Ensuring Y is also a float for BCEWithLogitsLoss\n",
    "    else:\n",
    "        Y = Y.to(device)\n",
    "\n",
    "    return X, Y, edge_index, edge_weight\n",
    "\n",
    "def prepare_masks(num_nodes):\n",
    "\n",
    "    train_index, temp_index = train_test_split(np.arange(num_nodes), test_size=0.3, random_state=42)\n",
    "    val_index, test_index = train_test_split(temp_index, test_size=0.6667, random_state=42)  # Adjusted test_size to split remaining 30% into 20% and 10%\n",
    "\n",
    "    # Create boolean masks for train, validation, and test datasets\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool).scatter_(0, torch.tensor(train_index), True)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool).scatter_(0, torch.tensor(val_index), True)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool).scatter_(0, torch.tensor(test_index), True)\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def train(model, data, train_mask, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.sigmoid(model(data)[mask])\n",
    "        preds = (preds > 0.5).float()\n",
    "        correct = (preds == data.y[mask]).float()\n",
    "        accuracy = correct.mean()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def plot_metrics(losses, val_accs):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))  # Set up the figure and one axis for the loss\n",
    "\n",
    "    # Plotting the training loss on the primary y-axis\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss', color='tab:blue')\n",
    "    ax1.plot(losses, label='Training Loss', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # Creating a second y-axis for validation accuracy\n",
    "    ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('Validation Accuracy', color='tab:red')\n",
    "    ax2.plot(val_accs, label='Validation Accuracy', color='tab:red')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # Adding a title and a grid\n",
    "    plt.title('Training Loss and Validation Accuracy over Epochs')\n",
    "    fig.tight_layout()  # Adjust the layout to make room for the second y-axis\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 65.8174, Val Acc: 0.4813\n",
      "Epoch: 002, Loss: 43.9873, Val Acc: 0.5645\n",
      "Epoch: 003, Loss: 30.4944, Val Acc: 0.6323\n",
      "Epoch: 004, Loss: 20.8445, Val Acc: 0.7075\n",
      "Epoch: 005, Loss: 13.6070, Val Acc: 0.7751\n",
      "Epoch: 006, Loss: 10.4740, Val Acc: 0.7950\n",
      "Epoch: 007, Loss: 8.4477, Val Acc: 0.8101\n",
      "Epoch: 008, Loss: 8.1600, Val Acc: 0.7809\n",
      "Epoch: 009, Loss: 8.7966, Val Acc: 0.7854\n",
      "Epoch: 010, Loss: 8.1523, Val Acc: 0.8133\n",
      "Epoch: 011, Loss: 7.4477, Val Acc: 0.8168\n",
      "Epoch: 012, Loss: 7.0656, Val Acc: 0.8213\n",
      "Epoch: 013, Loss: 6.6923, Val Acc: 0.8222\n",
      "Epoch: 014, Loss: 6.3530, Val Acc: 0.8208\n",
      "Epoch: 015, Loss: 6.1306, Val Acc: 0.8219\n",
      "Epoch: 016, Loss: 6.0093, Val Acc: 0.8213\n",
      "Epoch: 017, Loss: 5.8362, Val Acc: 0.8216\n",
      "Epoch: 018, Loss: 5.5674, Val Acc: 0.8190\n",
      "Epoch: 019, Loss: 5.2645, Val Acc: 0.8133\n",
      "Epoch: 020, Loss: 5.0230, Val Acc: 0.8058\n",
      "Epoch: 021, Loss: 4.8259, Val Acc: 0.7990\n",
      "Epoch: 022, Loss: 4.5967, Val Acc: 0.7900\n",
      "Epoch: 023, Loss: 4.3184, Val Acc: 0.8058\n",
      "Epoch: 024, Loss: 3.9285, Val Acc: 0.8133\n",
      "Epoch: 025, Loss: 3.5487, Val Acc: 0.8123\n",
      "Epoch: 026, Loss: 3.3449, Val Acc: 0.7923\n",
      "Epoch: 027, Loss: 3.2453, Val Acc: 0.7727\n",
      "Epoch: 028, Loss: 3.1284, Val Acc: 0.8064\n",
      "Epoch: 029, Loss: 2.8665, Val Acc: 0.8145\n",
      "Epoch: 030, Loss: 2.7043, Val Acc: 0.8189\n",
      "Epoch: 031, Loss: 2.5556, Val Acc: 0.8185\n",
      "Epoch: 032, Loss: 2.3939, Val Acc: 0.8150\n",
      "Epoch: 033, Loss: 2.2439, Val Acc: 0.8128\n",
      "Epoch: 034, Loss: 2.1342, Val Acc: 0.8140\n",
      "Epoch: 035, Loss: 2.0390, Val Acc: 0.8122\n",
      "Epoch: 036, Loss: 1.9390, Val Acc: 0.8088\n",
      "Epoch: 037, Loss: 1.8334, Val Acc: 0.8080\n",
      "Epoch: 038, Loss: 1.7114, Val Acc: 0.8124\n",
      "Epoch: 039, Loss: 1.5834, Val Acc: 0.8166\n",
      "Epoch: 040, Loss: 1.4708, Val Acc: 0.8186\n",
      "Epoch: 041, Loss: 1.3783, Val Acc: 0.8159\n",
      "Epoch: 042, Loss: 1.2823, Val Acc: 0.8142\n",
      "Epoch: 043, Loss: 1.1814, Val Acc: 0.8177\n",
      "Epoch: 044, Loss: 1.0976, Val Acc: 0.8185\n",
      "Epoch: 045, Loss: 1.0304, Val Acc: 0.8149\n",
      "Epoch: 046, Loss: 0.9923, Val Acc: 0.8086\n",
      "Epoch: 047, Loss: 0.9630, Val Acc: 0.8105\n",
      "Epoch: 048, Loss: 0.9236, Val Acc: 0.8128\n",
      "Epoch: 049, Loss: 0.8887, Val Acc: 0.8091\n",
      "Epoch: 050, Loss: 0.8593, Val Acc: 0.8087\n",
      "Epoch: 051, Loss: 0.8321, Val Acc: 0.8040\n",
      "Epoch: 052, Loss: 0.8052, Val Acc: 0.8042\n",
      "Epoch: 053, Loss: 0.7658, Val Acc: 0.8035\n",
      "Epoch: 054, Loss: 0.7265, Val Acc: 0.8004\n",
      "Epoch: 055, Loss: 0.6934, Val Acc: 0.7968\n",
      "Epoch: 056, Loss: 0.6636, Val Acc: 0.7948\n",
      "Epoch: 057, Loss: 0.6364, Val Acc: 0.7963\n",
      "Epoch: 058, Loss: 0.6109, Val Acc: 0.7961\n",
      "Epoch: 059, Loss: 0.5926, Val Acc: 0.7978\n",
      "Epoch: 060, Loss: 0.5778, Val Acc: 0.7971\n",
      "Epoch: 061, Loss: 0.5669, Val Acc: 0.7955\n",
      "Epoch: 062, Loss: 0.5531, Val Acc: 0.7964\n",
      "Epoch: 063, Loss: 0.5349, Val Acc: 0.7968\n",
      "Epoch: 064, Loss: 0.5239, Val Acc: 0.8152\n",
      "Epoch: 065, Loss: 0.5105, Val Acc: 0.8154\n",
      "Epoch: 066, Loss: 0.5011, Val Acc: 0.8156\n",
      "Epoch: 067, Loss: 0.4905, Val Acc: 0.8155\n",
      "Epoch: 068, Loss: 0.4774, Val Acc: 0.8131\n",
      "Epoch: 069, Loss: 0.4716, Val Acc: 0.8136\n",
      "Epoch: 070, Loss: 0.4642, Val Acc: 0.8139\n",
      "Epoch: 071, Loss: 0.4561, Val Acc: 0.8140\n",
      "Epoch: 072, Loss: 0.4464, Val Acc: 0.8289\n",
      "Epoch: 073, Loss: 0.4373, Val Acc: 0.8283\n",
      "Epoch: 074, Loss: 0.4270, Val Acc: 0.8291\n",
      "Epoch: 075, Loss: 0.4198, Val Acc: 0.8287\n",
      "Epoch: 076, Loss: 0.4142, Val Acc: 0.8434\n",
      "Epoch: 077, Loss: 0.4066, Val Acc: 0.8450\n",
      "Epoch: 078, Loss: 0.3993, Val Acc: 0.8452\n",
      "Epoch: 079, Loss: 0.3933, Val Acc: 0.8453\n",
      "Epoch: 080, Loss: 0.3874, Val Acc: 0.8449\n",
      "Epoch: 081, Loss: 0.3820, Val Acc: 0.8448\n",
      "Epoch: 082, Loss: 0.3748, Val Acc: 0.8454\n",
      "Epoch: 083, Loss: 0.3681, Val Acc: 0.8453\n",
      "Epoch: 084, Loss: 0.3628, Val Acc: 0.8444\n",
      "Epoch: 085, Loss: 0.3567, Val Acc: 0.8424\n",
      "Epoch: 086, Loss: 0.3519, Val Acc: 0.8434\n",
      "Epoch: 087, Loss: 0.3469, Val Acc: 0.8453\n",
      "Epoch: 088, Loss: 0.3419, Val Acc: 0.8451\n",
      "Epoch: 089, Loss: 0.3398, Val Acc: 0.8447\n",
      "Epoch: 090, Loss: 0.3384, Val Acc: 0.8446\n",
      "Epoch: 091, Loss: 0.3380, Val Acc: 0.8441\n",
      "Epoch: 092, Loss: 0.3381, Val Acc: 0.8440\n",
      "Epoch: 093, Loss: 0.3380, Val Acc: 0.8444\n",
      "Epoch: 094, Loss: 0.3351, Val Acc: 0.8462\n",
      "Epoch: 095, Loss: 0.3320, Val Acc: 0.8458\n",
      "Epoch: 096, Loss: 0.3299, Val Acc: 0.8456\n",
      "Epoch: 097, Loss: 0.3276, Val Acc: 0.8454\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     plot_metrics(losses, val_accs)\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     main()\n",
      "\u001b[1;32m/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m500\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, data, train_mask, optimizer, criterion)\n\u001b[0;32m---> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     val_acc \u001b[39m=\u001b[39m evaluate(model, data, val_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     val_accs\u001b[39m.\u001b[39mappend(val_acc)\n",
      "\u001b[1;32m/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(model(data)[mask])\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     preds \u001b[39m=\u001b[39m (preds \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     correct \u001b[39m=\u001b[39m (preds \u001b[39m==\u001b[39m data\u001b[39m.\u001b[39my[mask])\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     x, edge_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index\n\u001b[0;32m---> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project/GNN_Course_Sample_of_Patient/e_PLP_GNN.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch_geometric/nn/conv/sage_conv.py:134\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    131\u001b[0m     x \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mrelu(), x[\u001b[39m1\u001b[39m])\n\u001b[1;32m    133\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    135\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_l(out)\n\u001b[1;32m    137\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate.py:158\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m    152\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    153\u001b[0m         out,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect(\n\u001b[1;32m    159\u001b[0m         edge_index,\n\u001b[1;32m    160\u001b[0m         x,\n\u001b[1;32m    161\u001b[0m         mutable_size,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.sage_conv_SAGEConv_propagate.py:76\u001b[0m, in \u001b[0;36mcollect\u001b[0;34m(self, edge_index, x, size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(_x_0, Tensor):\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_size(size, \u001b[39m0\u001b[39m, _x_0)\n\u001b[0;32m---> 76\u001b[0m     x_j \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_select(_x_0, edge_index_j)\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     x_j \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py:300\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39mindex_select(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim, index)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_select_safe(src, index)\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py:304\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[0;34m(self, src, index)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_index_select_safe\u001b[39m(\u001b[39mself\u001b[39m, src: Tensor, index: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    303\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[1;32m    305\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    306\u001b[0m         \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m index\u001b[39m.\u001b[39mmin() \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X, Y, edge_index, edge_weight = load_data(device)\n",
    "    data = Data(x=X, y=Y, edge_index=edge_index, edge_attr=edge_weight).to(device)\n",
    "    num_features = X.size(1)\n",
    "    num_classes = Y.size(1)\n",
    "\n",
    "    model = SAGE_MLC(num_features, num_classes, hidden_channels=16).to(device)\n",
    "    # model = GCN_MLC(num_features, num_classes, hidden_channels=16).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    train_mask, val_mask, test_mask = prepare_masks(Y.size(0))\n",
    "\n",
    "    losses, val_accs = [], []\n",
    "    for epoch in range(1, 500):\n",
    "        loss = train(model, data, train_mask, optimizer, criterion)\n",
    "        val_acc = evaluate(model, data, val_mask)\n",
    "        losses.append(loss)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    test_acc = evaluate(model, data, test_mask)\n",
    "    print(f'Test Accuracy (0.6 train): {test_acc:.4f}')\n",
    "    plot_metrics(losses, val_accs)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
